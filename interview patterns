ðŸ”µ Round 1 â€” L1 Technical (SQL + Python)
ðŸŸ£ SQL
Window functions usage
Recursive CTEs
Identify numbers appearing three times consecutively

ðŸŸ£ Python
Coding on strings/lists/dicts
Concepts:
Decorators
Multiprocessing vs Multithreading

ðŸ”µ Round 2 â€” L2 Technical (Spark + PySpark)
ðŸŸ£ Spark Concepts
Broadcast vs Shuffle Join
Salting for skew handling
Bloom Filters
Spark memory internals (heap, GC)
Repartition vs Coalesce
SparkContext vs SparkSession
PySpark â†” Snowflake integration

ðŸŸ£ PySpark Coding
Joins + window functions using DataFrame API
Had to code in Notepad â€” no IDE support ðŸ˜…

ðŸ”µ Round 3 â€” Techno-Managerial Round
ðŸŸ£ Architecture & Projects
Discussed an end-to-end pipeline using:
Azure + Databricks + Airflow + Snowflake + Spark
Also asked to draw the data flow on draw.io.

ðŸŸ£ Scenario-Based Questions
Cost optimization on Spark clusters
Infra issues vs data issues
Delivering during resource shortages
Managing cross-team dependencies

ðŸ”µ Round 4 â€” HR Round
Salary discussion
Notice period
Role alignment
Work location flexibility

ðŸ’¡ What this interview teaches us:
âœ” SQL, PySpark, and Cloud Data Pipelines are non-negotiable
âœ” Real-world scenarios matter more than textbook definitions
âœ” Communication â†’ Architecture clarity â†’ Problem-solving = the real game
âœ” IDEs wonâ€™t save you â€” fundamentals will
